{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_folder = \"cifar-10-batches-py/\"\n",
    "batch1_filename = os.path.join(data_folder, \"data_batch_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def unpickle(filename):\n",
    "    with open(filename, 'rb') as fo:\n",
    "        return pickle.load(fo, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1 = unpickle(batch1_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 100\n",
    "image = batch1['data'][image_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.reshape((32,32, 3), order='F')\n",
    "import numpy as np\n",
    "image = np.rot90(image, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ece9419080>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "from theano import tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = T.dscalar()\n",
    "b = T.dscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = T.sqrt(a ** 2 + b ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = theano.function([a,b], c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(5.)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data.astype(np.float32)\n",
    "y_true = iris.target.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_true, random_state=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasagne\n",
    "input_val = T.fmatrix(\"inputs\")\n",
    "target_val = T.ivector(\"targets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = lasagne.layers.InputLayer(shape=X_train.shape, input_var=input_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = lasagne.layers.DenseLayer(input_layer, num_units=12, nonlinearity=lasagne.nonlinearities.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = lasagne.layers.DenseLayer(hidden_layer, num_units=3, nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_val = lasagne.layers.get_output(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lasagne.objectives.categorical_crossentropy(output_val, target_val)\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = lasagne.layers.get_all_params(output_layer, trainable=True)\n",
    "updates = lasagne.updates.sgd(loss, all_params, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = theano.function([input_val, target_val], loss, updates=updates, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_output = theano.function([input_val], output_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(1000):\n",
    "    train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_output = get_output(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 2 1 0 1 0 1 2 0 2 2 0 1 0 2 2 1 0 0 0 1 0 2 0 1 1 0 0 1 1 0 1 0 2\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_test, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from skimage.transform import resize\n",
    "from skimage import transform as tf\n",
    "from skimage.measure import label, regionprops\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_captcha(text, shear=0, size=(100,24)):\n",
    "    im = Image.new(\"L\", size, \"black\")\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    font = ImageFont.truetype(r\"Coval.otf\", 22)\n",
    "    draw.text((2, 2), text, fill=1, font=font)\n",
    "    image = np.array(im)\n",
    "    affine_tf = tf.AffineTransform(shear=shear)\n",
    "    image = tf.warp(image, affine_tf)\n",
    "    return image / image.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_image(image):\n",
    "    labeled_image = label(image > 0)\n",
    "    subimages = []\n",
    "    for region in regionprops(labeled_image):\n",
    "        start_x, start_y, end_x, end_y = region.bbox\n",
    "        subimages.append(image[start_x:end_x, start_y:end_y])\n",
    "    if len(subimages) == 0:\n",
    "        return [image,]\n",
    "    return subimages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = check_random_state(14)\n",
    "letters = list(\"ACBDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "shear_values = np.arange(0, 0.5, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(random_state=None):\n",
    "    random_state = check_random_state(random_state)\n",
    "    letter = random_state.choice(letters)\n",
    "    shear = random_state.choice(shear_values)\n",
    "    return create_captcha(letter, shear=shear, size=(20, 20)), letters.index(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, targets = zip(*(generate_sample(random_state) for i in range(3000)))\n",
    "dataset = np.array(dataset, dtype='float')\n",
    "targets = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quekai\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "onehot = OneHotEncoder()\n",
    "y = onehot.fit_transform(targets.reshape(targets.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.todense().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array([resize(segment_image(sample)[0], (20, 20)) for\n",
    "sample in dataset])\n",
    "X = dataset.reshape((dataset.shape[0], dataset.shape[1] *\n",
    "dataset.shape[2]))\n",
    "X = X / X.max()\n",
    "X = X.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quekai\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9, random_state=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lasagne import layers\n",
    "layers=[\n",
    "    ('input', layers.InputLayer),\n",
    "    ('hidden', layers.DenseLayer),\n",
    "    ('output', layers.DenseLayer),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quekai\\Anaconda3\\lib\\site-packages\\theano\\gpuarray\\dnn.py:184: UserWarning: Your cuDNN version is more recent than Theano. If you encounter problems, try updating Theano or downgrading cuDNN to a version >= v5 and <= v7.\n",
      "  warnings.warn(\"Your cuDNN version is more recent than \"\n"
     ]
    }
   ],
   "source": [
    "from lasagne import updates\n",
    "from nolearn.lasagne import NeuralNet\n",
    "from lasagne.nonlinearities import sigmoid, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = NeuralNet(layers=layers,\n",
    "                input_shape=X.shape,\n",
    "                hidden_num_units=100,\n",
    "                output_num_units=26,\n",
    "                hidden_nonlinearity=sigmoid,\n",
    "                 output_nonlinearity=softmax,\n",
    "                 hidden_b=np.zeros((100,), dtype=np.float64),\n",
    "                 update=updates.momentum,\n",
    "                 update_learning_rate=0.9,\n",
    "                 update_momentum=0.1,\n",
    "                 regression=True,\n",
    "                 max_epochs=1000,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quekai\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:426: FutureWarning: You should specify a value for 'n_splits' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(NSPLIT_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=None,\n",
       "     batch_iterator_test=<nolearn.lasagne.base.BatchIterator object at 0x000001ECEBC1C080>,\n",
       "     batch_iterator_train=<nolearn.lasagne.base.BatchIterator object at 0x000001ECED4864A8>,\n",
       "     check_input=True, custom_scores=None,\n",
       "     hidden_b=array([0., 0., ..., 0., 0.]),\n",
       "     hidden_nonlinearity=<function sigmoid at 0x000001ECEC241598>,\n",
       "     hidden_num_units=100, input_shape=(3000, 400),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('hidden', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=None, max_epochs=1000, more_params={},\n",
       "     objective=<function objective at 0x000001ECED6E5400>,\n",
       "     objective_loss_function=<function squared_error at 0x000001ECEC2BF2F0>,\n",
       "     on_batch_finished=[], on_epoch_finished=[], on_training_finished=[],\n",
       "     on_training_started=[],\n",
       "     output_nonlinearity=<function softmax at 0x000001ECEC24F950>,\n",
       "     output_num_units=26, regression=True, scores_train=[],\n",
       "     scores_valid=[],\n",
       "     train_split=<nolearn.lasagne.base.TrainSplit object at 0x000001ECED6EC080>,\n",
       "     update=<function momentum at 0x000001ECEC2BFC80>,\n",
       "     update_learning_rate=0.9, update_momentum=0.1,\n",
       "     use_label_encoder=False, verbose=0,\n",
       "     y_tensor_type=TensorType(float64, matrix))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003194888178913738\n"
     ]
    }
   ],
   "source": [
    "y_pred = net1.predict(X_test)\n",
    "y_pred = y_pred.argmax(axis=1)\n",
    "assert len(y_pred) == len(X_test)\n",
    "if len(y_test.shape) > 1:\n",
    "    y_test = y_test.argmax(axis=1)\n",
    "print(f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "batches = []\n",
    "for i in range(1, 6):\n",
    "    batch_filename = os.path.join(data_folder, \"data_batch_{}\".format(i))\n",
    "    batches.append(unpickle(batch1_filename))\n",
    "#     break    #IMPORTANT -- see chapter for explanation of this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([batch['data'] for batch in batches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X) / X.max()\n",
    "X = X.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quekai\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "y = np.hstack(batch['labels'] for batch in batches).flatten()\n",
    "y = OneHotEncoder().fit_transform(y.reshape(y.shape[0],1)).todense()\n",
    "y = y.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 3, 32, 32)\n",
    "X_test = X_test.reshape(-1, 3, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lasagne import layers\n",
    "layers=[\n",
    "        ('input', layers.InputLayer),\n",
    "        ('conv1', layers.Conv2DLayer),\n",
    "        ('pool1', layers.MaxPool2DLayer),\n",
    "        ('conv2', layers.Conv2DLayer),\n",
    "        ('pool2', layers.MaxPool2DLayer),\n",
    "        ('conv3', layers.Conv2DLayer),\n",
    "        ('pool3', layers.MaxPool2DLayer),\n",
    "        ('hidden4', layers.DenseLayer),\n",
    "        ('hidden5', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nolearn.lasagne import NeuralNet\n",
    "from lasagne.nonlinearities import sigmoid, softmax\n",
    "nnet = NeuralNet(layers=layers,\n",
    "                 input_shape=(None, 3, 32, 32),\n",
    "                 conv1_num_filters=32,\n",
    "                 conv1_filter_size=(3, 3),\n",
    "                 conv2_num_filters=64,\n",
    "                 conv2_filter_size=(2, 2),\n",
    "                 conv3_num_filters=128,\n",
    "                 conv3_filter_size=(2, 2),\n",
    "                 pool1_pool_size=(2,2),\n",
    "                 pool2_pool_size=(2,2),\n",
    "                 pool3_pool_size=(2,2),\n",
    "                 hidden4_num_units=500,\n",
    "                 hidden5_num_units=500,\n",
    "                 output_num_units=10,\n",
    "                 output_nonlinearity=softmax,\n",
    "                 update_learning_rate=0.01,\n",
    "                 update_momentum=0.9,\n",
    "                 regression=True,\n",
    "                 max_epochs=100,\n",
    "                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quekai\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:426: FutureWarning: You should specify a value for 'n_splits' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(NSPLIT_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 874058 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name     size\n",
      "---  -------  --------\n",
      "  0  input    3x32x32\n",
      "  1  conv1    32x30x30\n",
      "  2  pool1    32x15x15\n",
      "  3  conv2    64x14x14\n",
      "  4  pool2    64x7x7\n",
      "  5  conv3    128x6x6\n",
      "  6  pool3    128x3x3\n",
      "  7  hidden4  500\n",
      "  8  hidden5  500\n",
      "  9  output   10\n",
      "\n",
      "  epoch    trn loss    val loss    trn/val  dur\n",
      "-------  ----------  ----------  ---------  ------\n",
      "      1     \u001b[36m0.08996\u001b[0m     \u001b[32m0.08995\u001b[0m    1.00011  57.41s\n",
      "      2     \u001b[36m0.08992\u001b[0m     \u001b[32m0.08991\u001b[0m    1.00011  57.77s\n",
      "      3     \u001b[36m0.08988\u001b[0m     \u001b[32m0.08987\u001b[0m    1.00010  57.27s\n",
      "      4     \u001b[36m0.08984\u001b[0m     \u001b[32m0.08983\u001b[0m    1.00011  58.50s\n",
      "      5     \u001b[36m0.08979\u001b[0m     \u001b[32m0.08978\u001b[0m    1.00013  58.04s\n",
      "      6     \u001b[36m0.08974\u001b[0m     \u001b[32m0.08973\u001b[0m    1.00018  57.98s\n",
      "      7     \u001b[36m0.08968\u001b[0m     \u001b[32m0.08966\u001b[0m    1.00023  58.20s\n",
      "      8     \u001b[36m0.08960\u001b[0m     \u001b[32m0.08957\u001b[0m    1.00031  59.36s\n",
      "      9     \u001b[36m0.08950\u001b[0m     \u001b[32m0.08946\u001b[0m    1.00043  59.26s\n",
      "     10     \u001b[36m0.08937\u001b[0m     \u001b[32m0.08932\u001b[0m    1.00058  58.38s\n",
      "     11     \u001b[36m0.08921\u001b[0m     \u001b[32m0.08914\u001b[0m    1.00078  57.96s\n",
      "     12     \u001b[36m0.08900\u001b[0m     \u001b[32m0.08890\u001b[0m    1.00108  58.14s\n",
      "     13     \u001b[36m0.08871\u001b[0m     \u001b[32m0.08857\u001b[0m    1.00157  58.15s\n",
      "     14     \u001b[36m0.08832\u001b[0m     \u001b[32m0.08811\u001b[0m    1.00231  57.75s\n",
      "     15     \u001b[36m0.08775\u001b[0m     \u001b[32m0.08745\u001b[0m    1.00339  57.73s\n",
      "     16     \u001b[36m0.08697\u001b[0m     \u001b[32m0.08656\u001b[0m    1.00466  57.75s\n",
      "     17     \u001b[36m0.08597\u001b[0m     \u001b[32m0.08550\u001b[0m    1.00548  57.66s\n",
      "     18     \u001b[36m0.08489\u001b[0m     \u001b[32m0.08446\u001b[0m    1.00511  57.76s\n",
      "     19     \u001b[36m0.08392\u001b[0m     \u001b[32m0.08359\u001b[0m    1.00400  58.10s\n",
      "     20     \u001b[36m0.08314\u001b[0m     \u001b[32m0.08290\u001b[0m    1.00286  58.12s\n",
      "     21     \u001b[36m0.08251\u001b[0m     \u001b[32m0.08234\u001b[0m    1.00204  58.47s\n",
      "     22     \u001b[36m0.08196\u001b[0m     \u001b[32m0.08183\u001b[0m    1.00155  58.00s\n",
      "     23     \u001b[36m0.08142\u001b[0m     \u001b[32m0.08132\u001b[0m    1.00122  58.02s\n",
      "     24     \u001b[36m0.08087\u001b[0m     \u001b[32m0.08079\u001b[0m    1.00104  58.12s\n",
      "     25     \u001b[36m0.08030\u001b[0m     \u001b[32m0.08022\u001b[0m    1.00103  58.71s\n",
      "     26     \u001b[36m0.07968\u001b[0m     \u001b[32m0.07960\u001b[0m    1.00105  58.36s\n",
      "     27     \u001b[36m0.07902\u001b[0m     \u001b[32m0.07894\u001b[0m    1.00107  58.40s\n",
      "     28     \u001b[36m0.07832\u001b[0m     \u001b[32m0.07823\u001b[0m    1.00122  58.40s\n",
      "     29     \u001b[36m0.07761\u001b[0m     \u001b[32m0.07751\u001b[0m    1.00119  58.37s\n",
      "     30     \u001b[36m0.07688\u001b[0m     \u001b[32m0.07681\u001b[0m    1.00088  58.29s\n",
      "     31     \u001b[36m0.07614\u001b[0m     \u001b[32m0.07609\u001b[0m    1.00063  58.40s\n",
      "     32     \u001b[36m0.07540\u001b[0m     \u001b[32m0.07539\u001b[0m    1.00006  58.64s\n",
      "     33     \u001b[36m0.07465\u001b[0m     \u001b[32m0.07469\u001b[0m    0.99945  58.00s\n",
      "     34     \u001b[36m0.07391\u001b[0m     \u001b[32m0.07401\u001b[0m    0.99865  58.63s\n",
      "     35     \u001b[36m0.07316\u001b[0m     \u001b[32m0.07331\u001b[0m    0.99801  57.85s\n",
      "     36     \u001b[36m0.07244\u001b[0m     \u001b[32m0.07266\u001b[0m    0.99690  58.21s\n",
      "     37     \u001b[36m0.07173\u001b[0m     \u001b[32m0.07201\u001b[0m    0.99616  58.49s\n",
      "     38     \u001b[36m0.07103\u001b[0m     \u001b[32m0.07139\u001b[0m    0.99491  58.76s\n",
      "     39     \u001b[36m0.07035\u001b[0m     \u001b[32m0.07084\u001b[0m    0.99310  58.57s\n",
      "     40     \u001b[36m0.06971\u001b[0m     \u001b[32m0.07031\u001b[0m    0.99146  58.50s\n",
      "     41     \u001b[36m0.06909\u001b[0m     \u001b[32m0.06983\u001b[0m    0.98945  58.88s\n",
      "     42     \u001b[36m0.06848\u001b[0m     \u001b[32m0.06932\u001b[0m    0.98801  58.32s\n",
      "     43     \u001b[36m0.06787\u001b[0m     \u001b[32m0.06881\u001b[0m    0.98643  59.47s\n",
      "     44     \u001b[36m0.06730\u001b[0m     \u001b[32m0.06832\u001b[0m    0.98505  59.14s\n",
      "     45     \u001b[36m0.06675\u001b[0m     \u001b[32m0.06788\u001b[0m    0.98337  59.12s\n",
      "     46     \u001b[36m0.06620\u001b[0m     \u001b[32m0.06738\u001b[0m    0.98249  58.73s\n",
      "     47     \u001b[36m0.06566\u001b[0m     \u001b[32m0.06689\u001b[0m    0.98171  58.58s\n",
      "     48     \u001b[36m0.06513\u001b[0m     \u001b[32m0.06641\u001b[0m    0.98063  57.88s\n",
      "     49     \u001b[36m0.06460\u001b[0m     \u001b[32m0.06592\u001b[0m    0.97998  58.67s\n",
      "     50     \u001b[36m0.06409\u001b[0m     \u001b[32m0.06548\u001b[0m    0.97873  58.36s\n",
      "     51     \u001b[36m0.06358\u001b[0m     \u001b[32m0.06501\u001b[0m    0.97800  58.81s\n",
      "     52     \u001b[36m0.06305\u001b[0m     \u001b[32m0.06458\u001b[0m    0.97633  58.83s\n",
      "     53     \u001b[36m0.06253\u001b[0m     \u001b[32m0.06413\u001b[0m    0.97505  60.07s\n",
      "     54     \u001b[36m0.06200\u001b[0m     \u001b[32m0.06375\u001b[0m    0.97255  58.90s\n",
      "     55     \u001b[36m0.06147\u001b[0m     \u001b[32m0.06334\u001b[0m    0.97049  57.99s\n",
      "     56     \u001b[36m0.06096\u001b[0m     \u001b[32m0.06294\u001b[0m    0.96852  58.13s\n",
      "     57     \u001b[36m0.06044\u001b[0m     \u001b[32m0.06253\u001b[0m    0.96656  58.04s\n",
      "     58     \u001b[36m0.05992\u001b[0m     \u001b[32m0.06209\u001b[0m    0.96506  58.25s\n",
      "     59     \u001b[36m0.05938\u001b[0m     \u001b[32m0.06163\u001b[0m    0.96345  58.34s\n",
      "     60     \u001b[36m0.05884\u001b[0m     \u001b[32m0.06123\u001b[0m    0.96104  57.66s\n",
      "     61     \u001b[36m0.05830\u001b[0m     \u001b[32m0.06084\u001b[0m    0.95819  58.03s\n",
      "     62     \u001b[36m0.05775\u001b[0m     \u001b[32m0.06053\u001b[0m    0.95411  58.42s\n",
      "     63     \u001b[36m0.05720\u001b[0m     \u001b[32m0.06016\u001b[0m    0.95081  57.78s\n",
      "     64     \u001b[36m0.05664\u001b[0m     \u001b[32m0.05964\u001b[0m    0.94982  58.42s\n",
      "     65     \u001b[36m0.05607\u001b[0m     \u001b[32m0.05920\u001b[0m    0.94720  58.39s\n",
      "     66     \u001b[36m0.05548\u001b[0m     \u001b[32m0.05871\u001b[0m    0.94502  58.59s\n",
      "     67     \u001b[36m0.05489\u001b[0m     \u001b[32m0.05827\u001b[0m    0.94208  57.91s\n",
      "     68     \u001b[36m0.05427\u001b[0m     \u001b[32m0.05784\u001b[0m    0.93820  57.51s\n",
      "     69     \u001b[36m0.05365\u001b[0m     \u001b[32m0.05756\u001b[0m    0.93200  57.78s\n",
      "     70     \u001b[36m0.05301\u001b[0m     \u001b[32m0.05723\u001b[0m    0.92637  59.84s\n",
      "     71     \u001b[36m0.05237\u001b[0m     \u001b[32m0.05680\u001b[0m    0.92205  58.30s\n",
      "     72     \u001b[36m0.05172\u001b[0m     \u001b[32m0.05622\u001b[0m    0.91988  58.33s\n",
      "     73     \u001b[36m0.05108\u001b[0m     \u001b[32m0.05568\u001b[0m    0.91732  57.60s\n",
      "     74     \u001b[36m0.05041\u001b[0m     \u001b[32m0.05513\u001b[0m    0.91437  57.65s\n",
      "     75     \u001b[36m0.04973\u001b[0m     \u001b[32m0.05459\u001b[0m    0.91094  57.72s\n",
      "     76     \u001b[36m0.04906\u001b[0m     \u001b[32m0.05395\u001b[0m    0.90936  57.56s\n",
      "     77     \u001b[36m0.04837\u001b[0m     \u001b[32m0.05347\u001b[0m    0.90454  57.65s\n",
      "     78     \u001b[36m0.04766\u001b[0m     \u001b[32m0.05300\u001b[0m    0.89926  57.63s\n",
      "     79     \u001b[36m0.04696\u001b[0m     \u001b[32m0.05247\u001b[0m    0.89509  57.96s\n",
      "     80     \u001b[36m0.04625\u001b[0m     \u001b[32m0.05199\u001b[0m    0.88954  63.13s\n",
      "     81     \u001b[36m0.04553\u001b[0m     \u001b[32m0.05142\u001b[0m    0.88541  60.91s\n",
      "     82     \u001b[36m0.04480\u001b[0m     \u001b[32m0.05096\u001b[0m    0.87901  58.74s\n",
      "     83     \u001b[36m0.04406\u001b[0m     \u001b[32m0.05046\u001b[0m    0.87308  59.68s\n",
      "     84     \u001b[36m0.04332\u001b[0m     \u001b[32m0.04983\u001b[0m    0.86933  60.10s\n",
      "     85     \u001b[36m0.04259\u001b[0m     \u001b[32m0.04940\u001b[0m    0.86198  60.21s\n",
      "     86     \u001b[36m0.04184\u001b[0m     \u001b[32m0.04883\u001b[0m    0.85692  62.85s\n",
      "     87     \u001b[36m0.04109\u001b[0m     \u001b[32m0.04820\u001b[0m    0.85243  61.93s\n",
      "     88     \u001b[36m0.04032\u001b[0m     \u001b[32m0.04761\u001b[0m    0.84685  61.90s\n",
      "     89     \u001b[36m0.03955\u001b[0m     \u001b[32m0.04691\u001b[0m    0.84318  61.86s\n",
      "     90     \u001b[36m0.03879\u001b[0m     \u001b[32m0.04622\u001b[0m    0.83931  61.60s\n",
      "     91     \u001b[36m0.03802\u001b[0m     \u001b[32m0.04563\u001b[0m    0.83331  61.98s\n",
      "     92     \u001b[36m0.03727\u001b[0m     \u001b[32m0.04496\u001b[0m    0.82900  62.32s\n",
      "     93     \u001b[36m0.03652\u001b[0m     \u001b[32m0.04431\u001b[0m    0.82427  61.72s\n",
      "     94     \u001b[36m0.03576\u001b[0m     \u001b[32m0.04367\u001b[0m    0.81880  61.97s\n",
      "     95     \u001b[36m0.03501\u001b[0m     \u001b[32m0.04308\u001b[0m    0.81253  62.18s\n",
      "     96     \u001b[36m0.03427\u001b[0m     \u001b[32m0.04243\u001b[0m    0.80769  61.79s\n",
      "     97     \u001b[36m0.03352\u001b[0m     \u001b[32m0.04178\u001b[0m    0.80228  62.14s\n",
      "     98     \u001b[36m0.03277\u001b[0m     \u001b[32m0.04130\u001b[0m    0.79353  62.42s\n",
      "     99     \u001b[36m0.03202\u001b[0m     \u001b[32m0.04072\u001b[0m    0.78624  62.77s\n",
      "    100     \u001b[36m0.03126\u001b[0m     \u001b[32m0.04007\u001b[0m    0.78021  61.79s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=None,\n",
       "     batch_iterator_test=<nolearn.lasagne.base.BatchIterator object at 0x000001ECEBC1C080>,\n",
       "     batch_iterator_train=<nolearn.lasagne.base.BatchIterator object at 0x000001ECED4864A8>,\n",
       "     check_input=True, conv1_filter_size=(3, 3), conv1_num_filters=32,\n",
       "     conv2_filter_size=(2, 2), conv2_num_filters=64,\n",
       "     conv3_filter_size=(2, 2), conv3_num_filters=128, custom_scores=None,\n",
       "     hidden4_num_units=500, hidden5_num_units=500,\n",
       "     input_shape=(None, 3, 32, 32),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('conv1', <class 'lasagne.layers.conv.Conv2DLayer'>), ('pool1', <class 'lasagne.layers.pool.MaxPool2DLayer'>), ('conv2', <class 'lasagne.layers.conv.Conv2DLayer'>), ('pool2', <class 'lasagne.layers.pool.MaxPool2DLayer'>), ('conv3', <class..., <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=None, max_epochs=100, more_params={},\n",
       "     objective=<function objective at 0x000001ECED6E5400>,\n",
       "     objective_loss_function=<function squared_error at 0x000001ECEC2BF2F0>,\n",
       "     on_batch_finished=[],\n",
       "     on_epoch_finished=[<nolearn.lasagne.handlers.PrintLog object at 0x000001ECFDDEE160>],\n",
       "     on_training_finished=[],\n",
       "     on_training_started=[<nolearn.lasagne.handlers.PrintLayerInfo object at 0x000001ECFDDEE198>],\n",
       "     output_nonlinearity=<function softmax at 0x000001ECEC24F950>,\n",
       "     output_num_units=10, pool1_pool_size=(2, 2), pool2_pool_size=(2, 2),\n",
       "     pool3_pool_size=(2, 2), regression=True, scores_train=[],\n",
       "     scores_valid=[],\n",
       "     train_split=<nolearn.lasagne.base.TrainSplit object at 0x000001ECED6EC080>,\n",
       "     update=<function nesterov_momentum at 0x000001ECEC2BFD90>,\n",
       "     update_learning_rate=0.01, update_momentum=0.9,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(float64, matrix))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7405999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "y_pred = nnet.predict(X_test)\n",
    "print(f1_score(y_test.argmax(axis=1), y_pred.argmax(axis=1), average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
